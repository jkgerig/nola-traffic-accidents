{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all pickle files\n",
    "street_by_time = pd.read_pickle('../../data/interim/features/street_by_time.pickle')\n",
    "daylight = pd.read_pickle('../../data/interim/features/daylight.pickle')\n",
    "holidays = pd.read_pickle('../../data/interim/features/holiday_dates.pickle')\n",
    "street_info = pd.read_pickle('../../data/interim/features/streets_by_nhood.pickle')\n",
    "rush_hour = pd.read_pickle('../../data/interim/features/rush_hour.pickle')\n",
    "weather = pd.read_pickle('../../data/interim/features/weather.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Dataset for all accidents that is indexed by ROAD SEGMENT and DATETIME**\n",
    "\n",
    "1. Merge information from other datasets into accidents dataset\n",
    "2. Select 3 negative samples for each positive (accident) sample according to methodology from article\n",
    "3. Append negative samples to positive samples and clean up final output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_by_time['date'] = street_by_time.day_hour.dt.date\n",
    "street_by_time['hour'] = street_by_time.day_hour.dt.hour\n",
    "street_by_time = street_by_time[['date', 'hour', 'segment_id', 'accident_yn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_samples = street_by_time[['date', 'hour', 'segment_id']].copy()\n",
    "negative_sample_size = accident_samples.shape[0] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "sample_space_date = pd.Series(pd.date_range('2012-01-01', '2018-06-30')).dt.date\n",
    "sample_space_hour = pd.Series([x for x in range(24)])\n",
    "sample_space_location = street_info.segment_id.sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1000000\n",
    "sample_dates = np.random.choice(sample_space_date, size=n_rows)\n",
    "sample_hours = np.random.choice(sample_space_hour, size=n_rows)\n",
    "sample_locations = np.random.choice(sample_space_location, size=n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = pd.DataFrame(columns=['date', 'hour', 'segment_id'])\n",
    "\n",
    "generated_samples.date = sample_dates\n",
    "generated_samples.hour = sample_hours\n",
    "generated_samples.segment_id = sample_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = generated_samples.merge(accident_samples, how='left', indicator=True)\n",
    "negative_samples = negative_samples[negative_samples._merge == 'left_only']\n",
    "negative_samples = negative_samples.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sample_idx = np.random.choice(negative_samples.index, size=negative_sample_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_samples = negative_samples[negative_samples.index.isin(negative_sample_idx)]\n",
    "negative_samples = negative_samples[['date', 'hour', 'segment_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident_samples['accident_yn'] = 1\n",
    "negative_samples['accident_yn'] = 0\n",
    "\n",
    "all_samples = pd.concat([accident_samples, negative_samples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "daylight['sunrise'] = daylight.sunrise.ffill()\n",
    "daylight['sunset'] = daylight.sunset.ffill()\n",
    "daylight['dt_sunrise'] = pd.to_datetime(daylight.index.astype('str') + ' ' + daylight.sunrise.astype('str'))\n",
    "daylight['dt_sunset'] = pd.to_datetime(daylight.index.astype('str') + ' ' + daylight.sunset.astype('str'))\n",
    "daylight['dt_sunrise'] = daylight.dt_sunrise.dt.round('1h')\n",
    "daylight['dt_sunset'] = daylight.dt_sunset.dt.round('1h')\n",
    "daylight['date'] = daylight.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples['datetime'] = pd.to_datetime(all_samples.date.astype('str') + ' ' + all_samples.hour.astype('str') + ':00')\n",
    "all_samples = all_samples.merge(daylight, on='date')\n",
    "daylight_yn = np.where((all_samples['datetime'] >= all_samples['dt_sunrise']) & \\\n",
    "                       (all_samples['datetime'] <= all_samples['dt_sunset']), 1, 0)\n",
    "\n",
    "all_samples['daylight_yn'] = daylight_yn\n",
    "all_samples = all_samples[['date', 'hour', 'segment_id', 'accident_yn', 'datetime', 'daylight_yn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.DataFrame(holidays)\n",
    "holidays['dt_date'] = holidays.date.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = all_samples.merge(holidays, left_on='date', right_on='dt_date', how='left', indicator=True)\n",
    "holiday_yn = np.where(all_samples._merge == 'left_only', 0, 1)\n",
    "all_samples['holiday_yn'] = holiday_yn\n",
    "all_samples = all_samples[['date_x', 'hour', 'segment_id', 'accident_yn', 'datetime', 'daylight_yn', 'holiday_yn']]\n",
    "all_samples.columns = ['date', 'hour', 'segment_id', 'accident_yn', 'datetime', 'daylight_yn', 'holiday_yn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = all_samples.merge(rush_hour, left_on='datetime', right_index=True, how='left', indicator=True)\n",
    "rush_hour_yn = np.where(all_samples._merge == 'left_only', 0, 1)\n",
    "all_samples['rush_hour_yn'] = rush_hour_yn\n",
    "all_samples.drop(columns=['rush_hour', '_merge'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = all_samples.merge(weather, left_on='datetime', right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = street_info[['segment_id', 'shape_leng', 'class_Freeway', 'class_Local', 'class_Major Arterial', 'class_Other', 'class_Umimproved']]\n",
    "all_samples = all_samples.merge(streets, on='segment_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = ['date',\n",
    "                 'hour',\n",
    "                 'segment_id',\n",
    "                 'daylight_yn',\n",
    "                 'holiday_yn',\n",
    "                 'rush_hour_yn',\n",
    "                 'temp',\n",
    "                 'wind_speed',\n",
    "                 'precipitation',\n",
    "                 'shape_leng',\n",
    "                 'class_Freeway',\n",
    "                 'class_Local',\n",
    "                 'class_Major Arterial',\n",
    "                 'class_Other',\n",
    "                 'class_Umimproved',\n",
    "                 'accident_yn']\n",
    "\n",
    "all_samples = all_samples[final_columns]\n",
    "\n",
    "renamed_cols = ['date',\n",
    "                'hour',\n",
    "                'segment_id',\n",
    "                'daylight_yn',\n",
    "                'holiday_yn',\n",
    "                'rush_hour_yn',\n",
    "                'temp',\n",
    "                'wind_speed',\n",
    "                'precipitation',\n",
    "                'road_length',\n",
    "                'class_freeway',\n",
    "                'class_local',\n",
    "                'class_major',\n",
    "                'class_other',\n",
    "                'class_unimproved',\n",
    "                'accident_yn']\n",
    "\n",
    "all_samples.columns = renamed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples.to_pickle('../../data/processed/all_samples.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
